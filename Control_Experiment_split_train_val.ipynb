{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are data generation. Don't change. Training data are pairs of neighbor numbers. Validation data are pairs of random numbers.  \n",
    "The purpose of this setup is that if the model can generalize the relationship of the numbers from simple training data, then it is possible to do numerical reasoning whose training is on limited combination of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "int_pair_list = list(itertools.permutations(list(range(300)), 2))\n",
    "random.shuffle(int_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [x[0] for x in int_pair_list]\n",
    "x2 = [x[1] for x in int_pair_list]\n",
    "x1, x2 = np.array(x1), np.array(x2)\n",
    "y = (x1 >= x2) * 1 + (x1 > x2) * 1\n",
    "train_data = [x1[:70000], x2[:70000], y[:70000]]\n",
    "val_data = [x1[70000:], x2[70000:], y[70000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "BSZ = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the train and validate functions. Don't need to change unless want to try variations of training paradigm such as using multiclass SVM loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "def train(model, train_loader, val_loader, fail_tol, learning_rate=3e-4, label=\"\"):\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "#     criterion = torch.nn.MultiMarginLoss()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.1)\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    loss_list, val_acc_list = [], []\n",
    "    \n",
    "    fail_cnt, cur_best = 0, 0\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        avg_loss = 0.\n",
    "        res_cnt = Counter()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0:\n",
    "            for i, (x, labels) in enumerate(train_loader):\n",
    "\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(x)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "#                 res_cnt.update(list(predicted.squeeze().cpu().numpy()))\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "                obj = criterion(outputs, labels)\n",
    "                obj.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                optimizer.step()\n",
    "\n",
    "                avg_loss += obj.item() / len(train_loader)\n",
    "            \n",
    "            train_acc = (100 * correct / total)\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        val_acc_list.append(val_acc)\n",
    "        loss_list.append(avg_loss)\n",
    "\n",
    "        if (val_acc > cur_best):\n",
    "            print('Epoch: [{}/{}], Loss: {:.4}, Train acc: {:.4}, Val acc: {:.4}'.format(\n",
    "                epoch, num_epochs, avg_loss, train_acc, val_acc))\n",
    "            print(\"found best! save model...\")\n",
    "            torch.save(model.state_dict(), 'model' + \"-\" + label + '.ckpt')\n",
    "            cur_best = val_acc\n",
    "            fail_cnt = 0\n",
    "        else:\n",
    "            fail_cnt += 1\n",
    "            print('Epoch: [{}/{}], Loss: {:.4}, Train acc: {:.4}, Val acc: {:.4} ({}/{})'.format(\n",
    "                epoch, num_epochs, avg_loss, train_acc, val_acc, fail_cnt, fail_tol))\n",
    "        if fail_cnt > fail_tol:\n",
    "            return loss_list, val_acc_list\n",
    "\n",
    "#         scheduler.step(val_acc)\n",
    "    return loss_list, val_acc_list\n",
    "\n",
    "def test_model(loader, model):\n",
    "    from collections import Counter\n",
    "    res_cnt = Counter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x, labels in loader:\n",
    "        outputs = model(x)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "#         labels = labels.max(1)[1]\n",
    "#         res_cnt.update(list(predicted.squeeze().cpu().numpy()))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "#     print(\"test\", res_cnt)\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to input the pairs. Concatenation ($N\\times 2$) and subtraction ($N\\times 1$). Although subtraction should presumably be learned (with concatenation), it doesn't seem to work that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class numDataset(Dataset):\n",
    "    def __init__(self, data_list, device=DEVICE):\n",
    "        self.s1_list, self.s2_list, self.target_list = data_list\n",
    "        self.device = device\n",
    "        assert (len(self.s1_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "        \n",
    "    def __getitem__(self, key):    \n",
    "\n",
    "        s1_idx = self.s1_list[key]\n",
    "        s2_idx = self.s2_list[key]  \n",
    "        label = self.target_list[key]\n",
    "\n",
    "        return [s1_idx, s2_idx, label, self.device]\n",
    "    \n",
    "def collate_func(batch):\n",
    "    device = batch[0][3]\n",
    "    data_list, label_list = [], []\n",
    "    for datum in batch:\n",
    "        # Can change comma to minus (or minus to comma) in the next line\n",
    "        data_list.append([datum[0] , datum[1]])\n",
    "        label_list.append(datum[2])\n",
    "\n",
    "    return [torch.FloatTensor(np.array(data_list)).to(device), \n",
    "            torch.LongTensor(label_list).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = numDataset(train_data)\n",
    "val_dataset = numDataset(val_data)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BSZ,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BSZ,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using subtraction, input is 1d. Concatenation is 2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcNet(nn.Module):\n",
    "    def __init__(self, n_layers, fc_hid_dim, device=DEVICE):\n",
    "        super(fcNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc_hid_dim = fc_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Specify the input dimension here\n",
    "        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(2, fc_hid_dim), nn.ReLU())]+\n",
    "                                     [nn.Sequential(nn.Linear(fc_hid_dim, fc_hid_dim), nn.ReLU())] * (n_layers-2)+\n",
    "                                     [nn.Linear(fc_hid_dim, 3)])\n",
    "        self.init_weights()\n",
    "    def forward(self, x):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [layer if type(layer) == torch.nn.modules.linear.Linear else layer[0] for layer in self.linears]\n",
    "     \n",
    "        for layer in lin_layers:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is here. Feel free to change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 4\n",
    "fc_hid_dim = 32\n",
    "model = fcNet(n_layers, fc_hid_dim).to(DEVICE)\n",
    "res = train(model, train_loader, val_loader, 30, learning_rate=3e-5, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
